\section{MDPs as ADPs}

\begin{frame}{MDP as ADP}
A typical MDP with policy operator $T_\sigma$ is shown below
$$
T_\sigma v  = r_\sigma +\beta P_\sigma v
$$
We have the following in general
\begin{itemize}
    \item $V = \mathbb{R}^{\mathbf{X}}$ paired with pointwise partial order $\le$ is a poset
    \item $\mathbb{T}:=\{T_\sigma :\sigma\in\underbrace{\Sigma\neq \emptyset}_{G\neq \emptyset}\}$
    \item $\beta P_\sigma$ is positive hence $T_\sigma$ is order preserving
\end{itemize}
Hence, \textbf{MDPs} can be formulated into ADPs.
\end{frame}

\begin{frame}{ADPs generated by MDPs are well-posed}
Since we have $\rho(P_\sigma)=1$, hence $\rho(\beta P_\sigma)<1$. Each $T_\sigma\in\mathbb{T}$ has a unique fixed point in $\mathbb{R}^{\mathbf{X}}$ given by $(I-\beta P_\sigma)^{-1}r_\sigma$

Hence, the ADP is well-posed. And the fixed point is the lifetime value of the corresponding policy. 
    
\end{frame}

\begin{frame}{Exercise 2.1.1 ADP generated by MDP is order stable }
Show that $(\mathbb{R}^{\mathbf{X}},\mathbb{T}_{MDP})$ is order stable. 
\begin{proof}
    From the previous slide, we know that $(\mathbb{R}^{\mathbf{X}},\mathbb{T}_{MDP})$ is well-posed. Hence, fix $T_\sigma\in\mathbb{T}$, it has a unique fixed point $v_\sigma$. 
    
    Let $v\in V=\mathbb{R}^{\mathbf{X}}$ such that $v\le T_\sigma v$. Using the definition of $T_\sigma$, we have
    $$
    v\le r_\sigma + \beta P_\sigma v \implies  \underbrace{(I-\beta P_\sigma)v\le r_\sigma\implies v\le (I-\beta P_\sigma)^{-1}r_\sigma}_{Neumann } =v_\sigma
    $$
    This gives upward stability. Similarly, we can get downward stability.
\end{proof}
\end{frame}

\begin{frame}{Exercise 2.1.2}
Fix $v\in\mathbb{R}^{\mathbf{X}}$. Prove that if $\sigma\in\Sigma$ obeys
$$
\sigma(x) \in argmax_{a\in\Gamma(x)} \left\{r(x,a)+\beta\sum_{x'}v(x')P(x,a,x')\right\}\quad \text{for all $x\in\mathbf{X}$}
$$
then $\sigma$ is $v$-greedy; that is  $T_\sigma v\ge T_\tau v$ for all $\tau\in\Sigma$.
\begin{proof}
    We have 
    \begin{align*}
        T_\sigma v (x) &= r(x,\sigma (x)) + \beta\sum_{x'}v(x')P(x,\sigma(x),x')\\
        &\ge r(x,a) \beta\sum_{x'}v(x')P(x,a,x')\quad \text{for all $(x,a)$}
    \end{align*}
Hence, this implies $T_\sigma v\ge T_\tau v$ for all $\tau\in\Sigma$.
\end{proof}
\end{frame}

\begin{frame}{Exercise 2.1.3.}
Recall that the ADP Bellman operator is defined by the expression $Tv = \bigvee_\sigma T_\sigma v$. Show that, in the present setting, this can also be written as
\begin{equation*}
(T v)(x) = \max_{a\in\Gamma(x)} \left\{r(x,a) + \beta \sum_{x'} v(x')P(x,a,x') \right\} \quad (x \in X).
\end{equation*}
\begin{proof}
    skip
\end{proof}
\end{frame}


\begin{frame}{Exercise 2.1.4}
    Letting
\begin{equation}
M := \max_{(x,a)\in G} |r(x,a)| \quad \text{and} \quad \hat{V} := \left\{v \in \mathbb{R}^X : |v| \leq \frac{M}{1-\beta} \right\},
\end{equation}
show that every $T_\sigma$ is a self-map on $\hat{V}$. Show also that $v_\sigma \in \hat{V}$ for all $\sigma \in \Sigma$.

\begin{proof}
    We have
    \begin{align*}
    |T_\sigma v| & = |r_\sigma + \beta P_\sigma v|\\
    &\le |r_\sigma |+\beta |v|\\
    &\le M +\beta |v|\\
    &\le M+\beta (\frac{M}{1-\beta}) = \frac{M}{1-\beta}
    \end{align*}
\end{proof}
\end{frame}
