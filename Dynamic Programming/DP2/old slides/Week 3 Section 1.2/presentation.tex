\documentclass[11pt,xcolor={dvipsnames},hyperref={pdftex,pdfpagemode=UseNone,hidelinks,pdfdisplaydoctitle=true},usepdftitle=false]{beamer}
\usepackage{presentation}

% Enter presentation title to populate PDF metadata:
\hypersetup{pdftitle={DP2 Reading Group}}

% Enter path to PDF file with figures:
\newcommand{\pdf}{figures.pdf}

\begin{document}

% Enter title:
\title{DP2 Reading Group}

\information
%
% Enter URL to research paper (can be commented out):
[https://github.com/longye-tian]
%
% Enter authors:
{Longye Tian}
%
% Enter location and date (can be commented out):
{Sydney -- September 2024}

\frame{\titlepage}

% Enter content of presentation:

\begin{frame}
\frametitle{Definition of ADP}
\begin{definition}
We define an \textbf{abstract dynamic program (ADP)} to be a pair $(V, \mathbb{T})$, where
\begin{enumerate}
\item $V = (V, \precsim)$: partially ordered set and
\item $\mathbb{T} = \{T_\sigma: \sigma\in\Sigma\}$: nonempty family of order-preserving self-maps on $V$.
\end{enumerate}
In what follows,
\begin{itemize}
\item $V$ is called the \textbf{value space}
\item Each operator $T_\sigma\in\mathbb{T}$ is called a \textbf{policy operator}
\item $\Sigma$ is an arbitrary index set and elements of $\Sigma$ is called \textbf{policies}.
\item $v_\sigma\in V$ is a unique fixed of $T_\sigma$ and call it the \textbf{$\sigma$-value function}
\end{itemize}
\end{definition}
\end{frame}




\begin{frame}
\frametitle{Greedy Policies}
\begin{definition}
Let $(V,\mathbb{T})$ be an ADP with policy set $\Sigma$. Given $v\in V$, we say that
$$
\sigma\in\Sigma \text{ if \textbf{$v$-greedy} if $T_\tau v\precsim T_\sigma v$ for all $\tau\in\Sigma$}
$$
In other words, $\sigma$ is $v$-greedy if and only if $T_\sigma v$ is a greatest element of $\{T_\tau v: T_\tau\in\mathbb{T}\}$
\end{definition}
\begin{remark}
We let 
$$
V_G: =\{v\in V: \text{ at least one $v$-greedy policy exists}\}
$$
\end{remark}
\end{frame}


\begin{frame}
\frametitle{Bellman equation}
\begin{definition}
Let $(V,\mathbb{T})$ be an ADP with policy set $\Sigma$. We say that $v\in V$ satisfies the \textbf{Bellman Equation} if
$$
v: = \bigvee_{\sigma\in\Sigma} T_\sigma v\qquad(v\in V)
$$
We define the \textbf{Bellman operator generated by the ADP} via
$$
Tv:= \bigvee_{\sigma\in\Sigma} T_\sigma v \qquad\text{whenever the supremum exists}
$$
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Lemma 1.2.1.}
\begin{lemma}[Properties of Bellman Operator]
\begin{enumerate}
\item For $v\in V_G$, $T_\sigma v = Tv$ if and only if $\sigma\in\Sigma$ is $v$-greedy 
\begin{proof}
$(\implies)$\\
$T_\sigma v = Tv\implies T_\sigma v  = \bigvee_{\tau\in\Sigma} T_\tau v\implies T_\tau v\precsim T_\sigma v,\, \forall \tau\in \Sigma$ by definition, $\sigma$ is the $v$-greedy policy. $v\in V_G$ ensures existence.\\
$(\impliedby)$\\
By definition, $T_\tau v\precsim T_\sigma v,\,\forall \tau\in \Sigma$. Hence, there exists a greatest element hence supremum of $\{T_\tau v: T_\tau\in \mathbb{T}\}$. By definition, we have $T_\sigma v= Tv$.
\end{proof}
\end{enumerate}
\end{lemma}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.1. Continue}
\begin{lemma}[Properties of Bellman Operator]
\begin{enumerate}
\item[2.]For  $v\in V_G$, we have $T_\sigma v\precsim Tv$ for all $\sigma\in \Sigma$.
\begin{proof}
As $v\in V_G$, there exists a $v$-greedy policy, denoted it as $\tau$. Then, by part 1, we have by definition of $v$-greedy policy,
$$
T_\sigma v\precsim T_\tau v = Tv
$$
for all $\sigma\in \Sigma$.
\end{proof}
\end{enumerate}
\end{lemma}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.1 Continue}
\begin{lemma}[Properties of Bellman Operator]
\begin{enumerate}
\item[3.] $T$ is well-defined and order-preserving on $V_G$
\begin{proof}
For $v, w\in V_G$, there exists at least one $v$-greedy policy, denoted it as $\sigma$ and let $v\precsim w$.\\
From part 1, we know $Tv = T_\sigma v$. Hence, it is well-defined.\\
From definition of ADP, we know $T_\sigma$ is order-preserving, hence, we have,
$$
T v = T_\sigma v \precsim T_\sigma w \precsim Tw
$$
last step is by part 2. Hence $T$ is order-preserving on $V_G$.
\end{proof}
\end{enumerate}
\end{lemma}
\end{frame}

\begin{frame}
\frametitle{Properties of ADP}
We call $(V,\mathbb{T})$
\begin{itemize}
\item \textbf{well-posed} if each $T_\sigma\in \mathbb{T}$ has a unique fixed point in $V$
\item \textbf{regular} if $V_G=V$, i.e., if a $v$-greedy policy exists for every $v\in V$.
\item \textbf{bounded above} if there exists a $v\in V$ such that $T_\sigma v\precsim v$ for all $T_\sigma\in \mathbb{T}$
\item \textbf{downward stable} if each $T_\sigma\in\mathbb{T}$ is downward stable on $V$
\item \textbf{upward stable} if each $T_\sigma\in\mathbb{T}$ is upward stable on $V$
\item \textbf{order stable} if each $T_\sigma\in\mathbb{T}$ is order stable on $V$
\item \textbf{strongly order stable} if each $T_\sigma\in \mathbb{T}$ is strongly order stable on $V$
\item \textbf{order continuous} if each $T_\sigma\in \mathbb{T}$ is order continuous on $V$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.2}
\begin{lemma}
If $(V,\mathbb{T})$ is order continuous and $V$ is $\sigma$-chain complete. Then the Bellman operator $T$ is order continuous.
\end{lemma}
\begin{proof}
Let $v_n\uparrow v$. As $T$ is order preserving, $(Tv_n)$ is increasing.\\
Since $V$ is $\sigma$-chain complete, $\bigvee_n Tv_n \in V$. We want to show $\bigvee_n Tv_n =Tv$\\
First, we have $Tv_n\le Tv$ for all $n$ by order-preserving, this implies $\bigvee_n Tv_n \le Tv$. Hence, $Tv$ is an upper bound.\\ 
Let $w$ be an upper bound of $Tv_n$. Then we have $T_\sigma v_n \le w$ for all $\sigma\in\Sigma$ and for all $n$. Then, taking the supremum over $n$, we have $T_\sigma v \le w$ by order continuity of the ADP. Taking supremum over $\sigma\in\Sigma$, we have $Tv\le w$. Hence $Tv$ is the least upper bound.
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Exercise 1.2.3.}
Let $(V,\mathbb{T})$ be an ADP and let $V$ be $\sigma$-dedekind complete. Prove that if $(V,\mathbb{T})$ is order continuous and order stable, then $(V,\mathbb{T})$ is strongly order stable.
\begin{proof}
Implore the Tarski-Kantorovich I (Theorem 1.1.9). 
\end{proof}
\end{frame}


\begin{frame}
\frametitle{Subsets of the Value Space}
\begin{definition}
Let $(V,\mathbb{T})$ be an ADP. We oftern refer to the following three subsets of the value space
\begin{itemize}
\item $V_G:=$ all $v\in V$ such that at least one $v$-greedy policy exists
\item $V_U:=$ all $v\in V$ with $v\precsim Tv$
\item $V_\Sigma:=$ all $v\in V$ such that $T_\sigma v=v$ for some $T_\sigma\in\mathbb{T}$
\end{itemize}
\end{definition}
\end{frame}


\begin{frame}
\frametitle{Lemma 1.2.3}
\begin{lemma}
Let $(V,\mathbb{T})$ be regular, well-posed and upward stable. If $V_\Sigma$ has greatest element $v^*$, then $v\precsim v^*$ for all $v\in V_U$. 
\end{lemma}
\begin{proof}
Fix $v\in V_U$. By regularity, we let $\sigma$ be the $v$-greedy policy, then
$$
\underbrace{v\precsim Tv}_{v\in V_U}\underbrace{= T_\sigma v}_{Lemma 1.2.1}\underbrace{\implies v\le v_\sigma}_{\text{upward stable}} \underbrace{\le v^*}_{\text{greatest element}}
$$
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.4.}
\begin{lemma}
Let $(V,\mathbb{T})$ be an ADP. If $V_\Sigma\subset V_G$, then $V_\Sigma\subset V_U$
\end{lemma}
\begin{proof}
Fix $v\in V_\Sigma\subset V_G$. Let $v_\sigma$ be $\sigma$-value function and let $\tau$ be $v_\sigma$-greedy policy. By Lemma 1.2.1, we have
$$
v_\sigma= T_\sigma v_\sigma\precsim T_\tau v_\sigma = Tv_\sigma
$$
Hence, $v_\sigma\in V_U$ and $V_\Sigma\subset V_U$.
\end{proof}
\begin{remark}
Lemma 1.2.4. implies that if $(V,\mathbb{T})$ is well-posed, then $V_U\neq \emptyset$
\end{remark}
\end{frame}


\begin{frame}
\frametitle{Optimality and Bellman equation}
\begin{definition}
We say that a policy $\sigma\in\Sigma$ is \textbf{optimal} for $(V,\mathbb{T})$ if $v_\sigma$ is a greatest element of $V_\Sigma$.\\
In other words, $\sigma$ is optimal if it attains the highest possible lifetime value.
\end{definition}
\begin{definition}
Suppose $V_\Sigma$ has a greatest element $v^*$, which is called the \textbf{value function of the ADP}. We say that \textbf{Bellman's principle of optimality holds} if, for $\sigma\in \Sigma$,
$$
\sigma \text{ is optimal} \iff \sigma\text{ is the $v^*$-greedy policy}
$$
\end{definition}
\end{frame}

\begin{frame}
\frametitle{Three fundamental ADP optimality properties}
We say that the \textbf{fundamental ADP optimality properties holds} if
\begin{enumerate}
\item[(B1)] $V_\Sigma$ has a greatest element $v^*$
\item[(B2)] $v^*$ is the unique solution to the Bellman equation
\item[(B3)] Bellman's principle of optimality holds
\end{enumerate}
\begin{remark}
\begin{enumerate}
\item[(B1)] means that the ADP has a solution
\item[(B2)] characterize the solution
\item[(B3)] implies we can compute the solution by finding $v^*$-greedy policy
\item[*] (B1) and (B2) $\implies$ (B3)
\end{enumerate}
\end{remark}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.5.}
Let $(V,\mathbb{T})$ be a well-posed ADP. If $V_\Sigma$ has a greatest element $v^*$, the TFAE:
\begin{enumerate}
\item $v^*$ satisfies the Bellman equation, i.e., $v^* = \bigvee_\tau T_\tau v^*$
\item Bellman's principle of optimality holds
\end{enumerate}
\begin{proof}
$((1)\Rightarrow(2), \Rightarrow)$:\\
$T_{m} v^* \precsim \bigvee_\tau T_\tau v^* =v^*= T_\sigma v^*\forall T_m\in \mathbb{T}$\\
$((1)\Rightarrow(2), \Leftarrow)$:\\
$T_m v^* \precsim T_\sigma v^*\forall T_m\in\mathbb{T} \Rightarrow\bigvee_\tau T_\tau v^*\precsim T_\sigma v^* \Rightarrow v^*\precsim T_\sigma v^*\precsim \bigvee_\tau T_\tau v^*=v^*$
$((1)\Leftarrow(2))$:\\
Let $\sigma$ be $v^*$-greedy policy. Then, $T_\tau v^*\precsim T_\sigma v^*\forall T_\tau \in\mathbb{T}$\\
Hence, $v^*$ is an upper bound of $\{T_\tau v^*\}_{T_\tau \in\mathbb{T}}$. Let $\tilde v$ be any upper bound. Then, $v^*= T_\sigma v^* \precsim \tilde v$. Hence $v^* = \bigvee_\tau  T_\tau v^*$
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.6.}
Let $(V,\mathbb{T})$ be a well-posed ADP. Let $V_\Sigma$ have the greatest element $v^*$. If $v^*$ is unique fixed point of $T$ in $V$, then $\sigma\in\Sigma$ is optimal if and only if $T v_\sigma = v_\sigma$.
\begin{proof}
$(\Rightarrow)$\\
$v_\sigma = v^*\Rightarrow Tv_\sigma = Tv^* = v^*=v_\sigma$\\
$(\Leftarrow)$\\
$T v_\sigma = v_\sigma \Rightarrow v_\sigma=v^*$
\end{proof}
\end{frame}


\begin{frame}
\heading{Algorithms}
\end{frame}


\begin{frame}
\frametitle{HPI, OPI, VFI}
\begin{definition}
Let $(V,\mathbb{T})$ be a well-posed ADP with Bellman operator $T$.\\
The \textbf{Howard Policy Operator} corresponding to $(V,\mathbb{T})$ via\\
$H: V_G\to V_\Sigma,\qquad Hv = v_\sigma,\qquad\text{ $\sigma$ is $v$-greedy}$\\
For each $m\in\mathbb{N}$, the \textbf{optimistic policy operator}\\
$W_m: V_G\to V,\qquad W_mv:= T^m_\sigma v,\qquad\text{ where $\sigma$ is $v$-greedy}$\\
So that these maps are well-defined, we always select the same $v$-greedy policy when applying each to $v$.
\end{definition}
\end{frame}

\begin{frame}
\frametitle{Convergence}
Let $(V,\mathbb{T})$ be a well-posed ADP.  Suppose that the fundamental ADP optimality properties hold. Let $v^*$ denote the value function. We say that
\begin{itemize}
\item \textbf{VFI converges} if $T^nv\uparrow v^*$ for all $v\in V_U$
\item \textbf{OPI converges} if $W_m^nv\uparrow v^*$ for all $v\in V_U$ and all $m\in\mathbb{N}$
\item \textbf{HPI converges} if $H^nv\uparrow v^*$ for all $v\in V_U$
\item If, for all $v\in V_U$, there exists $n\in\mathbb{N}$ with $H^nv=v^*$, we say that \textbf{HPI converges in finitely many steps}. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Exercise 1.2.4}
Prove that convergence of OPI implies convergence of VFI.
\begin{proof}
Let $m=1$. We have $W_1 v = T_\sigma v = Tv=:v_1$, $W_1^2 v = W_1 v_1 = Tv_1= T^2 v$. By induction, we get $W_1^nv = T^nv$. Hence, we get OPI convergence implies VFI convergence.
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.7.}
If $(V,\mathbb{T})$ is regular and well-posed, then the following statement hold.
\begin{enumerate}
\item[L1] If $v\in V$ with $Hv=v$, then $Tv=v$
\begin{proof}
$Hv= v_\sigma =v\Rightarrow T v = T_\sigma v = T_\sigma v_\sigma = v_\sigma =v$
\end{proof}
\item[L2] The operators $T, W_m, H$ all maps $V_U$ to itself
\begin{proof}
$Tv\ge v$ and by $T$ is order-preserving on $V_G$ and by regularity, we get $T(Tv)\ge Tv$.\\
$W_m v = T_\sigma^m v = T_\sigma^{m-1}Tv\ge v$ by $v\in V_U$ and $T_\sigma^{m-1}$ is order-preserving and by $W_m$ is order-preserving, we get the claim.\\
$Hv = v_\sigma$. By regularity, we get $V_\Sigma\subset V_G\implies V_\Sigma\in V_U$.
\end{proof}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.7. Continue}
If $(V,\mathbb{T})$ is regular and well-posed, then the following statement hold.
\begin{enumerate}
\item[L3] The operator $W_m$ is order preserving on $V_U$ and
$$
v\in V_U\implies Tv\precsim W_m v\precsim T^m
$$
\end{enumerate}
\begin{proof}
Order preserving is from order-preserving of $T_\sigma$.\\
$W_m v = T_\sigma^m v = T_\sigma^{m-1}Tv = W{m-1} Tv$. The by $W_m$ is order preserving, we have $W_m v = W_{m-1} Tv\ge W_{m-1}v$. Iteratively, we get $W_m \ge Tv$.\\
For the second inequality, we have
$$
W_m v = T_\sigma^m v\precsim T_\sigma^{m-1} Tv\precsim T_\sigma^{m-2} T^2v
$$
Iteratively, we get the inequality.
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Lemma 1.2.8.}
If $(V,\mathbb{T})$ is well-posed, regular and upward stable, then for every $v\in V_U$, 
$$
v\precsim T^n v\precsim W_m^n v\precsim H^n v
$$
for all $n\in\mathbb{N}$. And the VFI sequence $(T^n v)$, OPI sequence $(W_m^n v)$ and HPI sequence $(H^n v)$ are all increasing.
\begin{corollary}
Let $(V,\mathbb{T})$ be regular and upward stable. If the fundamental ADP optimality properties hold, then convergence of VFI implies convergence of OPI and HPI.
\end{corollary}
\end{frame}

\begin{frame}
\heading{Optimality via Order Stability}
\end{frame}

\begin{frame}
\frametitle{Conditions for ADP fundamental properties hold}
\begin{theorem}
Let $(V,\mathbb{T})$ be well-posed, downward stable and $T$ has a fixed point in $V_G$, then
\begin{enumerate}
\item the fixed point of $T$ is the greatest element of $V_\Sigma$
\item $T$ has no other fixed point in $V_G$
\item $\sigma$ is optimal if and only if $\sigma$ is $v$-greedy.
\end{enumerate}
\end{theorem}
\end{frame}


\begin{frame}
\frametitle{Corollary 1.2.11}
Let $(V,\mathbb{T})$ be well-posed, regular and downward stable. If $T$ has a fixed point in $V$, then $T$ has exactly one fixed point in $V$ and moreover, the fundamental ADP optimality properties hold.
\end{frame}

\begin{frame}
\frametitle{Finite case - Theorem 1.2.12}
Let $(V,\mathbb{T})$ be well-posed, regular and order stable. If $\mathbb{T}$ is finite, then
\begin{enumerate}
\item the fundamental ADP optimality properties hold
\item HPI converges in finitely many steps
\end{enumerate}
\end{frame}

\begin{frame}
\heading{Optimality via Order Completeness}
\end{frame}

\begin{frame}
\frametitle{}




















\end{document}